{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8c0130",
   "metadata": {},
   "source": [
    "Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd83429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c807a",
   "metadata": {},
   "source": [
    "Set up API through Tweepy, define api calls function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e53c84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token = \"*****************\")\n",
    "\n",
    "\n",
    "query = 'ukraine -is:retweet lang:en'\n",
    "\n",
    "\n",
    "\"\"\"Tweets Fields requires public metrics to be inlcuded, wait time is in seconds to wait per api call, and conditional ensures\n",
    "tweets are not repeated, returns 3 arrs with metrics indexes corresponding to tweet indexes\"\"\"\n",
    "def return_api_fetch(q,calls, tweet_fields, max_results, wait_time = False, sort_order = \"recency\"):\n",
    "    tweets = []\n",
    "    shares_arr = []\n",
    "    view_arr = []\n",
    "    like_arr = []\n",
    "    for _ in range(0,calls):\n",
    "        tweets_api = client.search_recent_tweets(query=q, tweet_fields=[\"public_metrics\",  'created_at'], max_results=100,sort_order = sort_order)\n",
    "\n",
    "        for tweet in tweets_api.data:\n",
    "            if tweet.text not in tweets:\n",
    "                tweets.append(tweet.text)\n",
    "                shares_arr.append(tweet.public_metrics[\"retweet_count\"])\n",
    "                view_arr.append(tweet.public_metrics[\"impression_count\"])\n",
    "                like_arr.append(tweet.public_metrics[\"like_count\"])\n",
    "        if wait_time:\n",
    "            time.sleep(wait_time)\n",
    "    return tweets, shares_arr, view_arr, like_arr\n",
    "        \n",
    "tweets,shares,views,likes = return_api_fetch(q = query, calls = 10, tweet_fields = [\"public_metrics\",  'created_at'], max_results = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72d5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic Preprocessing function for tweets, removes hashtags as they interfere with KPE, stop words are also removed\"\"\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "stop_words = (stopwords.words('english'))\n",
    "\n",
    "\n",
    "def pre_process_tweet(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(\"[/\\()?;:,.*&^%$!1234567890-]\", \"\", string)\n",
    "    string = while_replace(string)\n",
    "    lis = string.split()\n",
    "    final = []\n",
    "    user_count = 0\n",
    "    for word in lis:\n",
    "        if word[0] == \"#\":\n",
    "            pass\n",
    "        elif word[0] == \"@\":\n",
    "            final.append(word)\n",
    "        elif (word.count(\"h\") >= 2 and word.count(\"a\") >= 2) or \"lol\" in word:\n",
    "            final.append(\"laugh\")\n",
    "\n",
    "        elif len(word) < 2 and word != \"i\" and word != \"a\":\n",
    "                pass\n",
    "        \n",
    "        elif \"http\" in word:\n",
    "            m = re.search('https?://([A-Za-z_0-9.-]+).*',word)\n",
    "            if m:\n",
    "                final.append(m.group(1))\n",
    "            else:\n",
    "                pass            \n",
    "        else:\n",
    "            if word not in stop_words:\n",
    "                final.append(word)\n",
    "    return final\n",
    "\n",
    "\"\"\"Sub function to remove characters that appear more than 3 times or double spaces\"\"\"\n",
    "def while_replace(tweet):\n",
    "    while '  ' in tweet:\n",
    "        tweet = tweet.replace('  ', ' ')\n",
    "    new = \"\"\n",
    "    for index in range(0,len(tweet)):\n",
    "        char = tweet[index]\n",
    "        if index == 0 or index == len(tweet)-1:\n",
    "            new += char\n",
    "        else:\n",
    "            if tweet[index-1] == char and tweet[index+1] == char:\n",
    "                pass\n",
    "            else:\n",
    "                \n",
    "                new += char\n",
    "        \n",
    "    return new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d7500e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocesses Tweets\"\"\"\n",
    "processed_tweets = []\n",
    "for tweet in tweets:\n",
    "    processed_tweets.append((pre_process_tweet(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b9c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defining nodes and connections for unweighted directional graph\"\"\"\n",
    "\n",
    "nodes = []\n",
    "for tweet in processed_tweets:\n",
    "    for word in tweet:\n",
    "        if word not in nodes:\n",
    "            nodes.append( word)\n",
    "bigrams = []\n",
    "for tweet in processed_tweets:\n",
    "    for i in range(len(tweet)-1):\n",
    "        pair = [tweet[i], tweet[i+1]]\n",
    "        if pair not in bigrams:\n",
    "            bigrams.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54f013f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating Network\"\"\"\n",
    "\n",
    "import networkx as nx\n",
    "graph = nx.DiGraph()\n",
    "graph.add_nodes_from(nodes)\n",
    "graph.add_edges_from(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4bf8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Scoring and ordering nodes with betweenness centrality\"\"\"\n",
    "node_scores_inbetween = nx.betweenness_centrality(graph)\n",
    "sorted_node_scores_inbetween = dict(sorted(node_scores_inbetween.items(), key=lambda item: -item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "277e21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Key phrase array creation code block, only phrases len >= 2 are kept, key words are top 100 ranked nodes\"\"\"\n",
    "key_phrase = []\n",
    "for tweet in processed_tweets:\n",
    "    temp = []\n",
    "    for word in tweet:\n",
    "        tag = True\n",
    "        if word in list(sorted_node_scores_inbetween.keys())[0:100]:\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            if len(temp) >= 2:\n",
    "                if temp not in key_phrase:\n",
    "                    key_phrase.append(temp)\n",
    "                    temp = []\n",
    "                else:\n",
    "                    temp = []\n",
    "            else:\n",
    "                temp = []\n",
    "    if len(temp) >= 2 and tag:\n",
    "        if temp not in key_phrase:\n",
    "            key_phrase.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7d40267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code block for ranking of key_phrase by likes, returns array of ranking metric\n",
    "#TODO move to function so that other metrics can be used easily\"\"\"\n",
    "\n",
    "\"\"\"Sub function to simply test if key_phrase is in a processed tweet\"\"\"\n",
    "def contains(small, big):\n",
    "    for i in range(len(big)-len(small)+1):\n",
    "        for j in range(len(small)):\n",
    "            if big[i+j] != small[j]:\n",
    "                break\n",
    "        else:\n",
    "            return i, i+len(small)\n",
    "    return False\n",
    "\n",
    "ranking = []\n",
    "for key_phrase_arr in key_phrase:\n",
    "    count = 0\n",
    "    for tweet in processed_tweets:\n",
    "        if contains(key_phrase_arr,tweet):\n",
    "            count += (likes[(processed_tweets.index(tweet))])\n",
    "    ranking.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7de40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
